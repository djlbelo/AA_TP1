Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the datasets provided, explain the need to standardize the attribute values.
R1: Attribute values need to be standardized in order to make all variables contribute equaly, because some features may have a wider range of values then others.



Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: To calculate the parameters for standardization we used sklearn.preprocessing StandardScaler, with this object we used, fit_transform for the train set and transform for the other sets
(validation, test). We only used fit_transform for the train set because fit computes the mean and standard deviation to be used for later scaling in other data sets.


Q3: Classification: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: To calculate the prior probability, we computed the logarithm of the number of occurrences of a given class and divided that number by the data size.


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: First we calculate the prior probability of each class, then for each attribute we used the KernelDensity object to score the samples of each attribute according to the features of the class
incrementing these scores to the probability. These results are then stored in a array that has to lists of values per class, we select the max from each corresponding index to determine the prediction.



Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: A low bandwith will have a high error, and if the bandwith is too high there can be overfitting.


Q6: Explain how you determined the best bandwidth parameter for your classifier. You may include a relevant piece of your code if this helps you explain.
R6: We tested the bandwidths from 0.02 to 0.6 in intervals of 0.02 and for each one we calculated the error for each fold and then we selected the bandwith with the best error.


Q7: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R7: Initially we start by optimizing the parameters, calculating the best parameter and storing it. Then, with the optimal parameter we re-initialize the classifier.
Finally, using the training data, we fit the classifier and we calculate the score and the predition with the training data.


Q8: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the one provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R8:

====================== ERROR ======================
Naive Bayes =  9.47030497592295 %
Gaussian Naive Bayes =  6.09951845906902 %

====================== ACCURACY ======================
Naive Bayes =  90.52969502407706 %
Gaussian Naive Bayes =  93.90048154093098 %

====================== BEST BANDWIDTH ======================
Best bandwith =  0.24 

====================== Normal Test ======================
Naive Bayes =  -0.9592139562547322  <  1.0  <  2.9592139562547324
Gaussian Naive Bayes =  -0.9592139562547322  <  1.0  <  2.9592139562547324 

====================== McNeman's Test ======================
Gaussian Naive Bayes VS Naive Bayes =  31.12962962962963


We can affirm, looking at the error and accuracy parameters, that the Gaussian Naive Bayes method is more efficient then then Naive Bayes method.

Q9: Regression: Explain what experiments and plots gave you good
evidence to choose a given model degree. 
R9: The best plot to choose the best degree is the plot with the validation and train error along the degree, because we can clearly see the efects of overfitting a model.


Q10: In the case of your mean squared error plot explain why one of the error
curves is always decreasing, while the other is not.
R10: In the mean squared error the train line decreases and the validation one increases because we are fitting the model to the train set, so the error naturaly decreases, while for the
validation set it increases because it is more likely to have errors while the model is fitted to the train set.


Q11: In the plots of the true versus predicted values, where would be
all the points when predicted by an ideal regressor? Justify.
R11: In the plots of the true versus predicted values, an ideal regressor would have all the points on the line (y = x), meaning that the predicted value is the same as the true value.


Q12: Explain your validation procedure and comment on the true error
of your chosen model for unseen data.
R12: To apply the validation procedure we fit the train set to the model from degree 1 to 6 and we calculated the error for the train set as weel as the validation set, and in the end we picked the best validation error.
The true error is a bit higher then the validation error because it is an unbiased estimate.


